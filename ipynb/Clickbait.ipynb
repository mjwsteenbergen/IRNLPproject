{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline #make_pipeline, make_union\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import SnowballStemmer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# \n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instances = pd.read_json(\"../data/instances.jsonl\", lines=True);\n",
    "df_truth = pd.read_json(\"../data/truth.jsonl\", lines=True); \n",
    "\n",
    "# Display first 5 rows of both\n",
    "display(df_instances.head())\n",
    "display(df_truth.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_truth.groupby('truthClass').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_number_regex = r\"[0-9]+\"\n",
    "is_word_regex = r\"[A-Za-z].*\"\n",
    "is_capital_word_regex = r\"[A-Z].*\"\n",
    "is_encoding_quot = r\"â€˜\"\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def cleanString(strin):\n",
    "    return strin#re.sub(is_encoding_quot, \"'\", strin)\n",
    "\n",
    "def extract_features(df):\n",
    "    def extract(df):\n",
    "        result = dict()\n",
    "        extract_from_title(cleanString(df[1]['targetTitle']), result)\n",
    "        #extract_from_article(df[1]['targetParagraphs'], result)\n",
    "        #extract_from_image(df[1]['postMedia'], result)\n",
    "        return result\n",
    "        \n",
    "    def extract_from_title(title, result):\n",
    "        tiny = title.strip().lower()\n",
    "        title_words = nltk.word_tokenize(tiny)\n",
    "        title_words_p = nltk.word_tokenize(title.strip())\n",
    "        title_words_stem = [stemmer.stem(word) for word in title_words]\n",
    "        title_words_number_repl = [re.sub(is_number_regex, \"[n]\", word) for word in title_words_stem]\n",
    "        \n",
    "        title_word_count = Counter(title_words_number_repl)\n",
    "        twrr_bigram_count = Counter(nltk.bigrams(title_words_number_repl))\n",
    "        \n",
    "        result.update({'word_in_title[{}]'.format(word): 1 for word, amount in title_word_count.items()})\n",
    "        #result['title_length'] = len(title)\n",
    "        #result['simple_title_words'] = len(title_words)\n",
    "        n_words = sum(1 for word in title_words_p if re.match(is_word_regex, word))\n",
    "        n_capital_words = sum(1 for word in title_words_p if re.match(is_capital_word_regex, word))\n",
    "        #result['words_starting_with_capital_count'] = sum(1 for word in title_words_p if re.match(is_capital_word_regex, word))\n",
    "        result['capital_vs_non_words_ratio'] = n_capital_words/n_words\n",
    "        #result['title_words'] = len(title.split(' '))\n",
    "        result['title_question_marks'] = 0 if title.find('?') == -1 else 1\n",
    "        pos_tag_count = Counter(tag for (word, tag) in nltk.pos_tag(title_words))\n",
    "        #result.update({'pos_tag[{}]'.format(tag): count for tag, count in pos_tag_count.items()})\n",
    "        #result['title_average_word_length'] = len(title) / result['title_words']\n",
    "        #pos_tag_bigram_count = Counter(nltk.bigrams(tag for (word, tag) in nltk.pos_tag(title_words)))\n",
    "        #result.update({'pos_tag_bigram[{}]'.format(bitag): count for bitag, count in pos_tag_bigram_count.items()})\n",
    "        result.update({'title_bigram[{}]'.format(bigram): count for bigram, count in twrr_bigram_count.items()})\n",
    "        return result\n",
    "    def extract_from_article(paragraphs, result):\n",
    "#         tiny = title.strip().lower()\n",
    "#         title_words = nltk.word_tokenize(tiny)\n",
    "        result['number_of_paragraphs'] = len(paragraphs)\n",
    "        entireArticle = ''.join(paragraphs)\n",
    "        result['article_length'] = len(entireArticle)\n",
    "        result['article_words'] = len(entireArticle.split(' '))\n",
    "        result['article_average_word_length'] = len(entireArticle) / len(entireArticle.split(' '))\n",
    "    \n",
    "        return result\n",
    "    def extract_from_image(postMedia, result):\n",
    "#         tiny = title.strip().lower()\n",
    "#         title_words = nltk.word_tokenize(tiny)\n",
    "        result['has_image'] = 1 if len(postMedia) > 0 else 0\n",
    "    \n",
    "        return result\n",
    "    return map(extract, df.iterrows())\n",
    "\n",
    "# def extract_features_titles(df):\n",
    "#     def extract_from_title(title):\n",
    "#         result = dict()\n",
    "#         tiny = title.strip().lower()\n",
    "#         title_words = nltk.word_tokenize(tiny)\n",
    "#         title_words_stem = [stemmer.stem(word) for word in title_words]\n",
    "#         title_words_number_repl = [re.sub(is_number_regex, \"[n]\", word) for word in title_words_stem]\n",
    "#         twrr_bigram_count = Counter(nltk.bigrams(title_words_number_repl))\n",
    "#         result['title_word_count'] = sum(1 for word in title_words if re.match(is_word_regex, word))\n",
    "#         result['title_token_count'] = len(title_words)\n",
    "#         pos_tag_count = Counter(tag for (word, tag) in nltk.pos_tag(title_words))\n",
    "#         result.update({'pos_tag[{}]'.format(tag): count for tag, count in pos_tag_count.items()})\n",
    "# #         result.update({'title_bigram[{}]'.format(bigram): count for bigram, count in twrr_bigram_count.items()})\n",
    "#         return result\n",
    "#     return map(extract_from_title, df['targetTitle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(extract_features_titles(df_instances.iloc[:5]['targetTitle']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickbaitClassifierNBA = Pipeline([\n",
    "    ('feature_extraction', FunctionTransformer(extract_features, validate=False)),\n",
    "    ('encoder', DictVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "clickbaitClassifierTree = Pipeline([\n",
    "    ('feature_extraction', FunctionTransformer(extract_features, validate=False)),\n",
    "    ('encoder', DictVectorizer()),\n",
    "    ('classifier', DecisionTreeClassifier(\n",
    "        min_samples_leaf=10,\n",
    "        criterion=\"entropy\"))\n",
    "])\n",
    "\n",
    "clickbaitClassifierXGB = Pipeline([\n",
    "    ('feature_extraction', FunctionTransformer(extract_features, validate=False)),\n",
    "    ('encoder', DictVectorizer()),\n",
    "    ('classifier', XGBClassifier())\n",
    "])\n",
    "\n",
    "clickbaitClassifierSVC = Pipeline([\n",
    "    ('feature_extraction', FunctionTransformer(extract_features, validate=False)),\n",
    "    ('encoder', DictVectorizer()),\n",
    "    ('classifier', LinearSVC(max_iter=4000))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "dummyClassifier = Pipeline([\n",
    "    ('feature_extraction', FunctionTransformer(extract_features, validate=False)),\n",
    "    ('encoder', DictVectorizer()),\n",
    "    ('classifier', DummyClassifier(strategy=\"most_frequent\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a split\n",
    "merged = pd.merge(df_instances, df_truth, on='id')\n",
    "X_train, X_test, y_train, y_test = train_test_split(merged, merged['truthClass'])\n",
    "display(X_train.head())\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickbaitClassifierNBA.fit(X_train, y_train);\n",
    "clickbaitClassifierTree.fit(X_train, y_train);\n",
    "clickbaitClassifierSVC.fit(X_train, y_train);\n",
    "#clickbaitClassifierXGB.fit(X_train, y_train);\n",
    "dummyClassifier.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tree = clickbaitClassifierTree.predict(X_test)\n",
    "pred_nb = clickbaitClassifierNBA.predict(X_test)\n",
    "pred_svc = clickbaitClassifierSVC.predict(X_test)\n",
    "#pred_xgb = clickbaitClassifierXGB.predict(X_test)\n",
    "pred_dummy = dummyClassifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds = [pred_tree, pred_nb, pred_svc, pred_xgb, pred_dummy]\n",
    "preds = [pred_tree, pred_nb, pred_svc, pred_dummy]\n",
    "#classifiers = [\"DecisionTree\", \"NaiveBayes\", \"SVC\", \"XGBoost\", \"Dummy\"]\n",
    "classifiers = [\"DecisionTree\", \"NaiveBayes\", \"SVC\", \"Dummy\"]\n",
    "\n",
    "truthmap = [['clickbait', 'no-clickbait'].index(item) for item in y_test]\n",
    "predsmap = [[['clickbait', 'no-clickbait'].index(item) for item in pred ] for pred in preds]\n",
    "\n",
    "precisions = [precision_score(truthmap, pred) for pred in predsmap]\n",
    "recalls = [recall_score(truthmap, pred) for pred in predsmap]\n",
    "accuracies = [accuracy_score(truthmap, pred) for pred in predsmap]\n",
    "cfm = [confusion_matrix(truthmap, pred) for pred in predsmap]\n",
    "\n",
    "pd.DataFrame({\"Classifier\": classifiers, \n",
    "               \"Accuracy\": accuracies,\n",
    "               \"Precision\": precisions,\n",
    "               \"Recall\": recalls,\n",
    "               \"CFM\": cfm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = clickbaitClassifierTree.named_steps['classifier']\n",
    "dv = clickbaitClassifierTree.named_steps['encoder']\n",
    "#tr = clickbaitClassifierXGB.named_steps['classifier']\n",
    "#dv = clickbaitClassifierXGB.named_steps['encoder']\n",
    "\n",
    "dfFeatureImportance = pd.DataFrame(list(zip(dv.feature_names_, tr.feature_importances_)))\n",
    "dfFeatureImportance.sort_values(1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train.iloc[4]['targetTitle'])\n",
    "display(cleanString(X_train.iloc[4]['targetTitle']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Digraph\n",
    "\n",
    "feature_regex = r\"X\\[([0-9]+)\\]\"\n",
    "\n",
    "features_lst = dv.feature_names_\n",
    "\n",
    "ttab = str.maketrans({\n",
    "        #'\\'': \"\",\n",
    "        #'[' : \"\",\n",
    "        #']' : \"\",\n",
    "        #'(' : \"\",\n",
    "        #')' : \"\",\n",
    "        #' ' : \"\",\n",
    "        #',' : \"\",\n",
    "        #'_' : \"\",\n",
    "        #'$' : \"\",\n",
    "        '\"' : \"\\\\\\\"\"\n",
    "    })\n",
    "\n",
    "def replace_feature_names(line):\n",
    "    def repl(match):\n",
    "        a = \"\\'{}\\'\".format(features_lst[int(match.group(1))].translate(ttab))\n",
    "        #print(a)\n",
    "        return a\n",
    "    linerxr = re.sub(feature_regex, repl, line)\n",
    "    return linerxr\n",
    "\n",
    "dotf = [replace_feature_names(line) for line in export_graphviz(tr).split('\\n')[1:-1]]\n",
    "\n",
    "Digraph(body=dotf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
