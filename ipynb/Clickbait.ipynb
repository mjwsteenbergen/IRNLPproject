{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer, Normalizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline #make_pipeline, make_union\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer #nltk.download('vader_lexicon')\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# \n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instances = pd.read_json(\"../fulldata/instances.jsonl\", lines=True, encoding='utf-8');\n",
    "df_truth = pd.read_json(\"../fulldata/truth.jsonl\", lines=True, encoding='utf-8'); \n",
    "# df_instances = pd.read_json(\"../data/instances.jsonl\", lines=True, encoding='utf-8');\n",
    "# df_truth = pd.read_json(\"../data/truth.jsonl\", lines=True, encoding='utf-8'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instances[df_instances[\"targetTitle\"].str.contains(\"’\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possibleTruthClasses = ['no-clickbait', 'clickbait']\n",
    "\n",
    "sampler = RandomUnderSampler()\n",
    "merged = pd.merge(df_instances, df_truth, on='id')\n",
    "merged[\"truthClassN\"] = list(map(possibleTruthClasses.index, merged[\"truthClass\"]))\n",
    "\n",
    "# Display first 5 rows\n",
    "# display(merged.head())\n",
    "\n",
    "# Resample to get equal distribution.\n",
    "sampled_X, sampled_y = sampler.fit_resample(merged, merged[\"truthClassN\"])\n",
    "sampled_X = pd.DataFrame(sampled_X, columns=merged.columns)\n",
    "\n",
    "# Show distribution of classes\n",
    "display(sampled_X.groupby('truthClass').count()[[\"id\"]])\n",
    "\n",
    "# Remove labels to avoid cheating\n",
    "del sampled_X[\"truthClass\"]\n",
    "del sampled_X[\"truthClassN\"]\n",
    "del sampled_X[\"truthJudgments\"]\n",
    "del sampled_X[\"truthMean\"]\n",
    "del sampled_X[\"truthMedian\"]\n",
    "del sampled_X[\"truthMode\"]\n",
    "\n",
    "display(sampled_X.head())\n",
    "display(sampled_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sampled_X, sampled_y)\n",
    "display(X_train.head())\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_number_regex = r\"[0-9]+\"\n",
    "is_word_regex = r\"[A-Za-z].*\"\n",
    "is_capital_word_regex = r\"[A-Z].*\"\n",
    "is_encoding_quot = r\"â€˜\"\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "sentimentAnalyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def cleanString(strin):\n",
    "    return strin#re.sub(is_encoding_quot, \"'\", strin)\n",
    "\n",
    "# def cleanString(strin):\n",
    "#     return re.sub(is_encoding_quot, \"'\", strin)\n",
    "\n",
    "def extract_features(df):\n",
    "    def extract(df):\n",
    "        result = dict()\n",
    "        extract_from_title(cleanString(df[1]['targetTitle']), result)\n",
    "        #extract_from_article(df[1]['targetParagraphs'], result)\n",
    "        #extract_from_image(df[1]['postMedia'], result)\n",
    "        return result\n",
    "        \n",
    "    def extract_from_title(title, result):\n",
    "        tiny = title.strip().lower()\n",
    "        title_words = nltk.word_tokenize(tiny)\n",
    "        title_words_p = nltk.word_tokenize(title.strip())\n",
    "        title_words_stem_pos_repl = [(re.sub(is_number_regex, \"[n]\", \n",
    "                                                 stemmer.stem(\n",
    "                                                     word.lower())), tag) \n",
    "                                     for (word, tag) in nltk.pos_tag(title_words_p)]\n",
    "        #title_words_stem = title_words\n",
    "        title_words_stem = [stemmer.stem(word) for word in title_words]\n",
    "        title_words_number_repl = [re.sub(is_number_regex, \"[n]\", word) for word in title_words_stem]\n",
    "        \n",
    "        pos_title_word_count = Counter(title_words_stem_pos_repl)\n",
    "        title_word_count = Counter(title_words_number_repl)\n",
    "        twrr_bigram_count = Counter(nltk.bigrams(title_words_number_repl))\n",
    "        \n",
    "        #result.update({'word_in_title[{}]'.format(word): amount for word, amount in title_word_count.items()})\n",
    "        result.update({'pos_word_in_title[{}]'.format(word): 1 for word, amount in pos_title_word_count.items()})\n",
    "        n_words = sum(1 for word in title_words_p if re.match(is_word_regex, word))\n",
    "        n_capital_words = sum(1 for word in title_words_p if re.match(is_capital_word_regex, word))\n",
    "        result['capital_vs_non_words_ratio'] = 0 if n_words == 0 else n_capital_words/n_words\n",
    "        #pos_tag_count = Counter(tag for (word, tag) in nltk.pos_tag(title_words))\n",
    "        pos_tag_count = Counter(tag for (word, tag) in nltk.pos_tag(title_words_p))\n",
    "        result['title_length'] = len(title)\n",
    "        result['simple_title_words'] = len(title_words)\n",
    "        result['title_words'] = len(title.split(' '))\n",
    "        result['title_question_marks'] = 0 if title.find('?') == -1 else 1\n",
    "        result.update({'pos_tag[{}]'.format(tag): count for tag, count in pos_tag_count.items()})\n",
    "        result['title_average_word_length'] = len(title) / result['title_words']\n",
    "        result.update({'title_bigram[{}]'.format(bigram): count for bigram, count in twrr_bigram_count.items()})\n",
    "        sentiment = sentimentAnalyzer.polarity_scores(title)\n",
    "        result['title_sent_neg'] = sentiment[\"neg\"]\n",
    "        result['title_sent_pos'] = sentiment[\"pos\"]\n",
    "        result['title_sent_neu'] = sentiment[\"neu\"]\n",
    "        return result\n",
    "    def extract_from_article(paragraphs, result):\n",
    "#         tiny = title.strip().lower()\n",
    "#         title_words = nltk.word_tokenize(tiny)\n",
    "        result['number_of_paragraphs'] = len(paragraphs)\n",
    "        entireArticle = ''.join(paragraphs)\n",
    "        result['article_length'] = len(entireArticle)\n",
    "        result['article_words'] = len(entireArticle.split(' '))\n",
    "        result['article_average_word_length'] = len(entireArticle) / len(entireArticle.split(' '))\n",
    "    \n",
    "        return result\n",
    "    def extract_from_image(postMedia, result):\n",
    "#         tiny = title.strip().lower()\n",
    "#         title_words = nltk.word_tokenize(tiny)\n",
    "        result['has_image'] = 1 if len(postMedia) > 0 else 0\n",
    "    \n",
    "        return result\n",
    "    return map(extract, df.iterrows())\n",
    "\n",
    "# def extract_features_titles(df):\n",
    "#     def extract_from_title(title):\n",
    "#         result = dict()\n",
    "#         tiny = title.strip().lower()\n",
    "#         title_words = nltk.word_tokenize(tiny)\n",
    "#         title_words_stem = [stemmer.stem(word) for word in title_words]\n",
    "#         title_words_number_repl = [re.sub(is_number_regex, \"[n]\", word) for word in title_words_stem]\n",
    "#         twrr_bigram_count = Counter(nltk.bigrams(title_words_number_repl))\n",
    "#         result['title_word_count'] = sum(1 for word in title_words if re.match(is_word_regex, word))\n",
    "#         result['title_token_count'] = len(title_words)\n",
    "#         pos_tag_count = Counter(tag for (word, tag) in nltk.pos_tag(title_words))\n",
    "#         result.update({'pos_tag[{}]'.format(tag): count for tag, count in pos_tag_count.items()})\n",
    "# #         result.update({'title_bigram[{}]'.format(bigram): count for bigram, count in twrr_bigram_count.items()})\n",
    "#         return result\n",
    "#     return map(extract_from_title, df['targetTitle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(extract_features_titles(df_instances.iloc[:5]['targetTitle']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickbaitClassifierNBA = Pipeline([\n",
    "    ('feature_extraction', FunctionTransformer(extract_features, validate=False)),\n",
    "    ('encoder', DictVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "clickbaitClassifierTree = Pipeline([\n",
    "    ('feature_extraction', FunctionTransformer(extract_features, validate=False)),\n",
    "    ('encoder', DictVectorizer()),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "clickbaitClassifierXGB = Pipeline([\n",
    "    ('feature_extraction', FunctionTransformer(extract_features, validate=False)),\n",
    "    ('encoder', DictVectorizer()),\n",
    "    ('classifier', XGBClassifier())\n",
    "])\n",
    "\n",
    "clickbaitClassifierSVC = Pipeline([\n",
    "    ('feature_extraction', FunctionTransformer(extract_features, validate=False)),\n",
    "    ('encoder', DictVectorizer()),\n",
    "    ('normalizer', Normalizer()),\n",
    "    ('classifier', LinearSVC(max_iter=4000))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "dummyClassifier = Pipeline([\n",
    "    ('feature_extraction', FunctionTransformer(extract_features, validate=False)),\n",
    "    ('encoder', DictVectorizer()),\n",
    "    ('classifier', DummyClassifier(strategy=\"most_frequent\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickbaitClassifierNBA.fit(X_train, y_train);\n",
    "clickbaitClassifierTree.fit(X_train, y_train);\n",
    "clickbaitClassifierSVC.fit(X_train, y_train);\n",
    "#clickbaitClassifierXGB.fit(X_train, y_train);\n",
    "dummyClassifier.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tree = clickbaitClassifierTree.predict(X_test)\n",
    "pred_nb = clickbaitClassifierNBA.predict(X_test)\n",
    "pred_svc = clickbaitClassifierSVC.predict(X_test)\n",
    "#pred_xgb = clickbaitClassifierXGB.predict(X_test)\n",
    "pred_dummy = dummyClassifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds = [pred_tree, pred_nb, pred_svc, pred_xgb, pred_dummy]\n",
    "preds = [pred_tree, pred_nb, pred_svc, pred_dummy]\n",
    "#classifiers = [\"DecisionTree\", \"NaiveBayes\", \"SVC\", \"XGBoost\", \"Dummy\"]\n",
    "classifiers = [\"DecisionTree\", \"NaiveBayes\", \"SVC\", \"Dummy\"]\n",
    "\n",
    "truthmap = y_test\n",
    "predsmap = preds\n",
    "#truthmap = [['clickbait', 'no-clickbait'].index(item) for item in y_test]\n",
    "#predsmap = [[['clickbait', 'no-clickbait'].index(item) for item in pred ] for pred in preds]\n",
    "\n",
    "precisions = [precision_score(truthmap, pred) for pred in predsmap]\n",
    "recalls = [recall_score(truthmap, pred) for pred in predsmap]\n",
    "accuracies = [accuracy_score(truthmap, pred) for pred in predsmap]\n",
    "cfm = [confusion_matrix(truthmap, pred) for pred in predsmap]\n",
    "\n",
    "pd.DataFrame({\"Classifier\": classifiers, \n",
    "               \"Accuracy\": accuracies,\n",
    "               \"Precision\": precisions,\n",
    "               \"Recall\": recalls,\n",
    "               \"CFM\": cfm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat([no_stemming_results, stemming_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = clickbaitClassifierTree.named_steps['classifier']\n",
    "dv = clickbaitClassifierTree.named_steps['encoder']\n",
    "#tr = clickbaitClassifierXGB.named_steps['classifier']\n",
    "#dv = clickbaitClassifierXGB.named_steps['encoder']\n",
    "\n",
    "dfFeatureImportance = pd.DataFrame(list(zip(dv.feature_names_, tr.feature_importances_)))\n",
    "dfOrdered = dfFeatureImportance.sort_values(1, ascending=False)\n",
    "display(dfOrdered[dfOrdered[1] > 0.00001].head(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(X_train.iloc[4]['targetTitle'])\n",
    "#display(cleanString(X_train.iloc[4]['targetTitle']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Digraph\n",
    "\n",
    "feature_regex = r\"X\\[([0-9]+)\\]\"\n",
    "value_regex = r\"value \\= \\[([0-9]+), ([0-9]+)\\]\"\n",
    "\n",
    "features_lst = dv.feature_names_\n",
    "\n",
    "ttab = str.maketrans({\n",
    "        #'\\'': \"\",\n",
    "        #'[' : \"\",\n",
    "        #']' : \"\",\n",
    "        #'(' : \"\",\n",
    "        #')' : \"\",\n",
    "        #' ' : \"\",\n",
    "        #',' : \"\",\n",
    "        #'_' : \"\",\n",
    "        #'$' : \"\",\n",
    "        '\"' : \"\\\\\\\"\"\n",
    "    })\n",
    "\n",
    "def replace_with_names(line):\n",
    "    def repl_feature(match):\n",
    "        a = \"\\'{}\\'\".format(features_lst[int(match.group(1))].translate(ttab))\n",
    "        #print(a)\n",
    "        return a\n",
    "    def repl_value(match):\n",
    "        a = \"Clickbait? No:{}, Yes:{}\".format(match.group(1),match.group(2))\n",
    "        #print(a)\n",
    "        return a\n",
    "    linerxr = re.sub(feature_regex, repl_feature, line)\n",
    "    linerxr = re.sub(value_regex, repl_value, linerxr)\n",
    "    return linerxr\n",
    "\n",
    "dotf = [replace_with_names(line) for line in export_graphviz(tr).split('\\n')[1:-1]]\n",
    "\n",
    "Digraph(body=dotf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pred_nb\n",
    "old_colwidth = pd.options.display.max_colwidth\n",
    "pd.options.display.max_colwidth = 200\n",
    "Xtst = X_test.copy()\n",
    "Xtst[\"predicted\"] = predicted\n",
    "tocmp = pd.merge(Xtst[[pred_i != corr_i for (pred_i, corr_i) in zip(predicted, y_test)]], merged, on=\"id\")\n",
    "display(tocmp[[\"id\", \"predicted\", \"truthClassN_y\", \"targetTitle_y\", \"postText_y\", \"truthMean\", \"truthMedian\", \"truthJudgments\"]])\n",
    "pd.options.display.max_colwidth = old_colwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
